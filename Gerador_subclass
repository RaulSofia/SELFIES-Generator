from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Softmax, Embedding
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
import os
from datetime import datetime
from Processador import Processador

class Gerador(Sequential):
    def __init__(self, vocab="auto"):
        super(Gerador, self).__init__()
        self.processador = Processador()

    def fit(self, smiles_train, size, epochs, batch_size, patience):
        self.processador = Processador(size, self.vocab)
        (x_train, y_train) = self.processador.processa(smiles=smiles_train)
        #checkpoint = ModelCheckpoint()
        #print(x_train[0], y_train[0])
        x_train = smiles_train
        early_stop = EarlyStopping(monitor='loss', patience=patience)
        callbacks_list = [early_stop]
        super(Gerador, self).fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True)#, callbacks=callbacks_list) #TODO

    



if __name__ == "__main__":
    gerador = Gerador()
    gerador.add(Embedding(input_dim=32, output_dim=256, input_length=None)) #TODO mesmo que em baixo
    gerador.add(LSTM(512, return_sequences=True, dropout=0.2))
    gerador.add(LSTM(512, return_sequences=True, dropout=0.2))
    gerador.add(LSTM(512, return_sequences=True, dropout=0.2))
    gerador.add(Dense(32)) #TODO tratar do len(vocab)
    gerador.add(Softmax())
    gerador.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=['accuracy'])
    gerador.summary()
    gerador.fit("ChEMBL_filtered.txt", 100000, 100, 3, 16, 5)
    gerador.save("teste.ger")
    #model.save("teste.ger")

